# -*- coding: utf-8 -*-
"""aten-fx-pass.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1aIUQX8l34h3WVu5Mq8J28r_fMnTV3fH6
"""

! pip uninstall -y -q torch
! pip install -q torch

! pip install rich
from rich import inspect

import torch
import torch.nn as nn

class MLP(nn.Module):
  def __init__(self):
    super().__init__()
    self.fc1 = nn.Linear(31, 99)
    self.fc2 = nn.Linear(99, 111)

  def forward(self, x):
    x = self.fc1(x)
    x = self.fc2(x)
    return x

model = MLP()

batch_size = 8
input = torch.randn(batch_size, 31)

def needs_padding(args):
  
  for arg in args:
    size = arg.meta["tensor_meta"].shape
    for i in size:

      # Fix this to proper alignment because
      # I need to also look at shape and dtype
      if not i // 2:
        return False
    
    return True

def nearest_larger_multiple_of_2(num):
  return (num + 1) // 2 * 2

from torch import fx

def pad_mm(fx_g: fx.GraphModule):
    new_graph = fx.Graph()
    env = {}
    for node in fx_g.graph.nodes:
        if node.target == torch.ops.aten.addmm.default:
          if needs_padding(node.args):
              print("THE SHAPE NOW IS ")
              size = int(tuple(env[node.args[0]].meta["tensor_meta"].shape)[0])
              print("THE SIZE IS")
              pad_amount = nearest_larger_multiple_of_2(size) - size
              new_a_pad = new_graph.call_function(torch.ops.aten.cat, (env[node.args[0]], torch.ops.aten.zeros([pad_amount, 1])))
              new_mm_pad = new_graph.call_function(torch.ops.aten.addmm.default, (new_a_pad, env[node.args[1]]))
              new_mm = new_graph.call_function(torch.ops.aten.slice, (new_mm_pad, 0, pad_amount))
          env[node] = new_mm
        
        else:
            new_node = new_graph.node_copy(node, lambda n: env[n])
            env[node] = new_node
    return fx.GraphModule(fx_g, new_graph)



import torch._dynamo
import torch._functorch as functorch
from torch._functorch.aot_autograd import aot_module_simplified

def toy_backend(gm, sample_inputs): 
    def my_compiler(gm, sample_inputs):

        gm2 = pad_mm(gm)
        # gm2.print_readable()
        gm2.print_readable()

        return gm.forward

    # Invoke AOTAutograd
    return aot_module_simplified(
        gm,
        sample_inputs,
        fw_compiler=my_compiler
    )

torch._dynamo.reset()
fn = torch.compile(backend=toy_backend, dynamic=True)(model)

# triggers compilation of forward graph on the first run
out = fn(input)

# triggers compilation of backward graph on the first run
# out.sum().backward()



